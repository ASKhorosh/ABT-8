{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964cd308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import hashlib\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter, OrderedDict\n",
    "from random import choices\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import presto  # https://github.com/prestosql/presto-python-client\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from IPython.display import display\n",
    "from pandas.io.json._normalize import nested_to_record\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "\n",
    "def _t_stat(a, b):\n",
    "    \"\"\"\n",
    "\tCalculates t-statistic for two Pandas.Series\n",
    "    \"\"\"\n",
    "    return (a.mean() - b.mean()) / np.sqrt(a.var() / len(a) + b.var() / len(b))\n",
    "\n",
    "\n",
    "def _bootstrap(df, measures, iters=5000, alpha=0.05, random_state=1, verbose=True):\n",
    "    \"\"\"\n",
    "    Checks whether [measures] means are approx. equal across (!) Test groups (!) compared to (!) Control 1 (!)\n",
    "\n",
    "    Returns a dictionary of this kind: {'ar': [0.6866], 'GH': [0.619], 'num_rides': [0.9132]}\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "\n",
    "    df:\n",
    "        Dataframe with column \"groups\" - obtained from _split_to_groups() function.\n",
    "    measures:\n",
    "        list of features to be compared across Test groups vs Control group\n",
    "    iters:\n",
    "        number of samples with replacements\n",
    "    alpha:\n",
    "        Type I error probability\n",
    "    verbose: Boolean - optional\n",
    "        whether to print annoying statements :)\n",
    "        \n",
    "    Sources:\n",
    "    1. Chapter 16 of Bradley Efron and Robert J. Tibshirani (1993) An Introduction to the Bootstrap. Boca Raton: Chapman & Hall/CRC.\n",
    "    2. https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Bootstrap_hypothesis_testing\n",
    "    \n",
    "    \"\"\"\n",
    "    random.seed(random_state)\n",
    "\n",
    "    pval_dictio = {}  # dictionary for measures` p-value\n",
    "    for measure in measures:\n",
    "\n",
    "        if verbose:\n",
    "            print(\"We are checking\", measure, \"feature\")\n",
    "\n",
    "        control = df[df[\"groups\"] == \"Control 1\"][measure]\n",
    "        # groups to be compared with Control 1\n",
    "        groups = list(\n",
    "            filter(lambda x: \"control 1\" not in x.lower(), df[\"groups\"].unique())\n",
    "        )\n",
    "        groups.sort()  # from Control ascending to Test ascending\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Sorted groups: {groups}\")\n",
    "\n",
    "        tests_vs_control_pval = []\n",
    "        for group in groups:\n",
    "            measure_data = df[measure]\n",
    "            test = df[df[\"groups\"] == group][measure]\n",
    "            # if control exceeds test, makes sense to test whether control is significantly better\n",
    "            sign = np.sign(test.mean() - control.mean())\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Initial means: {group}: {test.mean()}; Control 1: {control.mean()}\"\n",
    "                )\n",
    "\n",
    "            # if Test group metric has smaller value, we will change the test direction (see upper comment)\n",
    "            if sign < 0:\n",
    "                test, control = control, test\n",
    "\n",
    "            # 1\n",
    "            t_stat_h0 = _t_stat(test, control)\n",
    "            # 2\n",
    "            new_test = np.array(test - test.mean() + measure_data.mean())\n",
    "            new_control = np.array(control - control.mean() + measure_data.mean())\n",
    "\n",
    "            # Generating samples\n",
    "            bootstrap_Test = np.random.choice(\n",
    "                new_test, size=(iters, len(new_test)), replace=True\n",
    "            )\n",
    "            bootstrap_Control = np.random.choice(\n",
    "                new_control, size=(iters, len(new_control)), replace=True\n",
    "            )\n",
    "\n",
    "            # Averaging for each sample\n",
    "            x_bar_star = np.mean(bootstrap_Test, axis=1)\n",
    "            y_bar_star = np.mean(bootstrap_Control, axis=1)\n",
    "\n",
    "            # Variance for each sample\n",
    "            x_var_star = np.var(bootstrap_Test, axis=1)\n",
    "            y_var_star = np.var(bootstrap_Control, axis=1)\n",
    "\n",
    "            # t-stats\n",
    "            t_stats = (x_bar_star - y_bar_star) / np.sqrt(\n",
    "                x_var_star / len(new_test) + y_var_star / len(new_control)\n",
    "            )\n",
    "\n",
    "            # p_value\n",
    "            pvalue = sign * sum(t_stats >= t_stat_h0) / iters\n",
    "\n",
    "            tests_vs_control_pval.append(pvalue)\n",
    "\n",
    "            if verbose:\n",
    "                if abs(pvalue) <= alpha:\n",
    "                    print(\n",
    "                        f\"REJECT H0: there is significant difference between test and control. p-value: {pvalue}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(f\"FAIL TO REJECT H0. p-value: {pvalue}\")\n",
    "\n",
    "        pval_dictio[measure] = tests_vs_control_pval\n",
    "\n",
    "    return pval_dictio\n",
    "\n",
    "\n",
    "def _hash_generate_groups(x, n_buckets, random_state=1):\n",
    "    \"\"\"\n",
    "    Returns a random bucket for a measure, given overall number of buckets.\n",
    "    \"\"\"\n",
    "    random.seed(random_state)\n",
    "\n",
    "    return (\n",
    "        int(\n",
    "            re.sub(\n",
    "                \"[^0-9]\", \"\", hashlib.md5(\"{}\".format(x).encode(\"utf-8\")).hexdigest(),\n",
    "            )\n",
    "        )\n",
    "        + np.random.randint(0, n_buckets)\n",
    "    ) % n_buckets\n",
    "\n",
    "\n",
    "def _partial__bucket_test(\n",
    "    df, measures, n_buckets, alpha, plot, random_state=1, verbose=True\n",
    "):\n",
    "    \"\"\"Returns a dictionary with pvalues for each group for each measure.\"\"\"\n",
    "\n",
    "    assert \"groups\" in df.columns, Exception(\n",
    "        \"'groups' column with group names not found!\"\n",
    "    )\n",
    "\n",
    "    # creating buckets. (_hash_generate_groups() can be applied to any column)\n",
    "    df[\"_buckets\"] = df[\"groups\"].apply(\n",
    "        lambda x: _hash_generate_groups(\n",
    "            x, n_buckets=n_buckets, random_state=random_state\n",
    "        )\n",
    "    )\n",
    "    ttest_final = {}\n",
    "    for measure in measures:\n",
    "        if verbose:\n",
    "            print(f\"Exploring {measure}\")\n",
    "\n",
    "        # dataframe for further groups comparison\n",
    "        grouper = df.groupby([\"groups\", \"_buckets\"]).agg({measure: np.mean})\n",
    "        groups = df[\"groups\"].unique()\n",
    "        groups.sort()\n",
    "        if verbose:\n",
    "            print(f\"Groups order: {groups}\")\n",
    "\n",
    "        # getting p-values of the Normality test\n",
    "        normaltest_results = {\n",
    "            group: scipy.stats.normaltest(grouper.loc[group]).pvalue[0]\n",
    "            for group in groups\n",
    "        }\n",
    "        if verbose:\n",
    "            print(\"Normality test results: \", normaltest_results, \"\\n\")\n",
    "\n",
    "        if any(filter(lambda x: x < alpha, list(normaltest_results.values()))):\n",
    "            if verbose:\n",
    "                print(\n",
    "                    \"Failed to get to normal distribution for measure: {}\".format(\n",
    "                        measure\n",
    "                    )\n",
    "                )\n",
    "                print(\"Normality test results: \", normaltest_results)\n",
    "            return None\n",
    "\n",
    "        # CAUTION: ttest_ind() returns two-sided p-value, but we need one-sided\n",
    "        # so we divide by 2, but also need to know the sign of t-stat\n",
    "\n",
    "        # ttest_results = {group: scipy.stats.ttest_ind(grouper.loc['Control 1'], \\\n",
    "        #                                               grouper.loc[group]).pvalue[0] / 2 \\\n",
    "        #                  for group in groups if 'Control' not in group}\n",
    "\n",
    "        ttest_results = []\n",
    "        for group in [el for el in groups if \"control\" not in el.lower()]:\n",
    "            res = scipy.stats.ttest_ind(grouper.loc[\"Control 1\"], grouper.loc[group])\n",
    "            ttest_results.append(res.pvalue[0] * np.sign(res.statistic[0]) / 2)\n",
    "            assert res.pvalue >= 0, Exception(\"p-value is negative for some reason...\")\n",
    "        ttest_final[measure] = ttest_results\n",
    "\n",
    "    if plot:\n",
    "        for measure in measures:\n",
    "            plt.tight_layout()\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.subplot(len(measures), 1, measures.index(measure) + 1)\n",
    "            sns.distplot(df.groupby(\"_buckets\").agg({measure: np.mean})[measure])\n",
    "            plt.title(\"Bucketed {} distribution\".format(measure))\n",
    "            plt.xlabel(measure)\n",
    "            plt.ylabel(\"Density\")\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "    return ttest_final\n",
    "\n",
    "\n",
    "def _bucket_test(\n",
    "    df,\n",
    "    measures,\n",
    "    n_buckets=None,\n",
    "    min_obs_per_bucket=25,\n",
    "    min_buckets=100,\n",
    "    alpha=0.05,\n",
    "    plot=False,\n",
    "    max_iters=7,\n",
    "    random_state=1,\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a dictionary of dictionaries with a p-value\n",
    "        (based on the appropriate ranking test) for each measure for each group.\n",
    "\n",
    "    Parameters:\n",
    "    --------------\n",
    "    df:\n",
    "        Dataframe with column \"groups\" - obtained from _split_to_groups() function.\n",
    "    measures:\n",
    "        list of features to be compared across Test groups vs Control group\n",
    "    n_buckets: int\n",
    "        # buckets to split each metric into\n",
    "    min_obs_per_bucket:\n",
    "        minimal number of observations per bucket\n",
    "    min_buckets:\n",
    "        minimal number of buckets\n",
    "    alpha:\n",
    "        Type I error probability (used while getting to normal distribution)\n",
    "    plot: [True, False] - optional\n",
    "        whether to plot a \"bucketed\" distribution for each measure\n",
    "    max_iters:\n",
    "        maximal number of iterations allowed to try splitting into bucket\n",
    "        to get normal distribution\n",
    "    \"\"\"\n",
    "\n",
    "    assert isinstance(measures, list), Exception(\n",
    "        '\"measures\" argument must be of type: list'\n",
    "    )\n",
    "    assert \"groups\" in df.columns, Exception(\n",
    "        \"Column 'groups' with group names not found!\"\n",
    "    )\n",
    "\n",
    "    # INCORRECT LOGIC! NEED AN ASSERTION THAT ENOUGH OBSERVATIONS ARE PRESENT FOR EACH (!!!) GROUP!!! ALSO, ASSERT UNIQUE user_id COLUMN!!!\n",
    "    if any(\n",
    "        df.groupby(\"groups\").agg({measures[0]: len}).values.flatten()\n",
    "        < min_buckets * min_obs_per_bucket\n",
    "    ):\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"Too few observations to create {} buckets with at least {} obs. each. \\\n",
    "                \\nChange settings or use another test.\".format(\n",
    "                    min_buckets, min_obs_per_bucket\n",
    "                )\n",
    "            )\n",
    "        return None\n",
    "\n",
    "    n_buckets = n_buckets or min_buckets\n",
    "    if verbose:\n",
    "        print(\"> Buckets: \", n_buckets, \"\\n\")\n",
    "\n",
    "    ret = _partial__bucket_test(\n",
    "        df=df,\n",
    "        measures=measures,\n",
    "        n_buckets=n_buckets,\n",
    "        alpha=alpha,\n",
    "        plot=plot,\n",
    "        verbose=verbose,\n",
    "    )\n",
    "    num_iters = 0\n",
    "    while (not ret) and num_iters <= max_iters:\n",
    "        num_iters += 1\n",
    "        if verbose:\n",
    "            print(\"\\n> TRYING AGAIN...\\n\")\n",
    "        ret = _partial__bucket_test(\n",
    "            df=df,\n",
    "            measures=measures,\n",
    "            n_buckets=n_buckets,\n",
    "            alpha=alpha,\n",
    "            plot=plot,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "    if not ret:\n",
    "        if verbose:\n",
    "            print(\n",
    "                \"\\n> MAX_ITERS LIMIT REACHED! FAILED TO GET TO NORMAL DISTRIBUTION.\\\n",
    "            \\nINCREASE MAX_ITERS OR SKIP THE TEST.\"\n",
    "            )\n",
    "        return None\n",
    "    return ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
